{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from matplotlib import pyplot as plt\n",
    "from skgpytorch.models import ExactGPRegressor\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "torch.manual_seed(0)\n",
    "x_dist = Normal(torch.tensor([0.0]), torch.tensor([5.0]))\n",
    "X = x_dist.sample((N,))\n",
    "K = ScaleKernel(RBFKernel())\n",
    "K.base_kernel.lengthscale = 0.5\n",
    "K.outputscale = 4.0\n",
    "K_ = K(X,X) + (1**2)*torch.eye(len(X))\n",
    "dist = MultivariateNormal(torch.zeros((N,)),K_.evaluate())\n",
    "Y = dist.sample()\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDGP(ExactGPRegressor):\n",
    "    def __init__(self, train_x, train_y, mll):\n",
    "        super().__init__(train_x, train_y, mll)\n",
    "    \n",
    "    def sgd_fit(self, batch_size, lr, n_epochs, n_restarts, thetas, random_state):\n",
    "\n",
    "        # torch.manual_seed(random_state)\n",
    "        ## creating nn_indices\n",
    "        neigh = NearestNeighbors(n_neighbors=batch_size, algorithm='kd_tree')\n",
    "        neigh.fit(self.train_x.cpu())\n",
    "        _, neigh_idx = neigh.kneighbors(self.train_x.cpu(), batch_size)\n",
    "\n",
    "        if (len(self.train_x) % batch_size  == 0):\n",
    "          num_batches = int(len(self.train_x)/batch_size)\n",
    "        else:\n",
    "          num_batches = int((len(self.train_x)/batch_size)) + 1\n",
    "\n",
    "        least_loss = float(\"inf\")\n",
    "        best_mll_state = None\n",
    "        self.history[\"total_loss\"] = []\n",
    "        self.history[\"gradient\"] = []\n",
    "        self.mll.train()\n",
    "        for restart in range(n_restarts):\n",
    "            self.optimizer = torch.optim.SGD(self.mll.parameters(), lr=lr)\n",
    "            # self.optimizer = torch.optim.Adam(self.mll.parameters(), lr=0.1)\n",
    "            self.history[\"total_loss\"].append([])\n",
    "            self.history[\"gradient\"].append([])\n",
    "\n",
    "            # Resetting the model for restarts\n",
    "            if restart > 0:  \n",
    "                for param in self.mll.parameters():\n",
    "                    torch.nn.init.normal_(param, mean=0.0, std=1.0)\n",
    "            self.mll.model.initialize(**thetas)\n",
    "            self.mll.model.covar_module.base_kernel.raw_lengthscale.requires_grad = False\n",
    "            for epoch in range(1, n_epochs+1):\n",
    "                loss = 0\n",
    "                for iteration in range(1, num_batches+1):\n",
    "           \n",
    "                    # idx = torch.tensor((iteration - 1)%self.train_x.shape[0])\n",
    "                    idx =  torch.randint(\n",
    "                            low=0, high=self.train_x.shape[0], size=(1,)\n",
    "                        )[0]\n",
    "                    indices = neigh_idx[idx,]\n",
    "                    X_batch = self.train_x[indices,]\n",
    "                    y_batch = self.train_y[indices,]\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    batch_loss = self.loss_func(X_batch, y_batch)\n",
    "\n",
    "                    batch_loss.backward()\n",
    "                    self.history[\"gradient\"][restart].append(2*torch.log(torch.linalg.norm(torch.tensor([self.mll.model.covar_module.raw_outputscale.grad,self.mll.likelihood.noise_covar.raw_noise.grad]), ord=2)).item())\n",
    "                    loss += batch_loss.item()\n",
    "\n",
    "                    self.mll.model.covar_module.raw_outputscale.grad*= batch_size/(3*torch.log(torch.tensor(batch_size)))\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    for group in self.optimizer.param_groups:\n",
    "                        group['lr'] = lr/(iteration+1)\n",
    "                \n",
    "                loss = loss / num_batches\n",
    "                self.history[\"total_loss\"][restart].append(loss)\n",
    "\n",
    "            # Check if best loss\n",
    "            if loss < least_loss:\n",
    "                self.best_restart = restart\n",
    "                least_loss = loss\n",
    "                best_mll_state = self.mll.state_dict()\n",
    "\n",
    "        # Load the best model\n",
    "        if best_mll_state is not None:\n",
    "            self.mll.load_state_dict(best_mll_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "batch_sizes = [128,64,32]\n",
    "theta = {'likelihood.noise_covar.noise': torch.tensor(3).to(device),\n",
    "        'covar_module.base_kernel.lengthscale': torch.tensor(0.5).to(device),\n",
    "        'covar_module.outputscale': torch.tensor(5).to(device),}\n",
    "fig,ax = plt.subplots(1,3,figsize=(30,6))\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_dist = Normal(torch.tensor([0.0]), torch.tensor([5.0]))\n",
    "X = x_dist.sample((N,))\n",
    "K = ScaleKernel(RBFKernel())\n",
    "K.base_kernel.lengthscale = 0.5\n",
    "K.outputscale = 4.0\n",
    "cov = K(X,X) + (1.0)*(torch.eye(len(X)))\n",
    "dist = MultivariateNormal(torch.zeros((1024)),cov.evaluate())\n",
    "torch.manual_seed(5)\n",
    "Y = dist.sample()\n",
    "\n",
    "\n",
    "for j in range(len(batch_sizes)):\n",
    "    gradient = []\n",
    "    for i in range(10):\n",
    "        lr = 5.0\n",
    "        batch_size = batch_sizes[j]\n",
    "    \n",
    "        \n",
    "        kernel = ScaleKernel(RBFKernel(ard_num_dims=X.shape[1])).to(device)\n",
    "        model = SGDGP(X.to(device), Y.to(device), kernel).to(device)\n",
    "        model.sgd_fit(batch_size,lr,25,1,thetas = theta, random_state=0)\n",
    "        # print(torch.std_mean(torch.Tensor(model.history['gradient'][0]), unbiased = True, dim=0))\n",
    "        gradient.append(model.history['gradient'][0])\n",
    "    std,mean = torch.std_mean(torch.Tensor(gradient), dim=0)\n",
    "    # print(std, mean)\n",
    "    \n",
    "    ax[j].plot(mean, 'b',linewidth=0.5)\n",
    "    ax[j].fill_between(range(len(mean)),mean-0.5*std, mean + 0.5*std, alpha=0.5)\n",
    "    ax[j].set_xlabel('Iteration (k)')\n",
    "    ax[j].set_ylabel('log(del(L(O(k)))')\n",
    "    ax[j].set_title(f'(m= {batch_sizes[j]})')\n",
    "    ax[j].set_yticks([-4,-5,-6,-7,-8,-9])\n",
    "    ax[j].set_ylim([-10,-2])\n",
    "\n",
    "plt.savefig('figure2_sgd1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SGDGP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ac03bc53b81f274766a4c3d65a8bb144021040a3f00168f019aaf215aad8e7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
