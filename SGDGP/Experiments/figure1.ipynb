{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import distrax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install distrax\n",
    "  import distrax\n",
    "\n",
    "try:\n",
    "  import tinygp\n",
    "except ModuleNotFoundError:\n",
    "  %pip install tinygp\n",
    "  import tinygp\n",
    "try:\n",
    "  import jax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install jax \n",
    "  import jax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "  %pip install matplotlib \n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  import GPy\n",
    "except ModuleNotFoundError:\n",
    "  %pip install GPy\n",
    "  import GPy\n",
    "try:\n",
    "  from tqdm import tqdm\n",
    "except ModuleNotFoundError:\n",
    "  %pip install tqdm\n",
    "  from tqdm import tqdm\n",
    "try: \n",
    "  import jaxopt\n",
    "except ModuleNotFoundError:\n",
    "  %pip install jaxopt\n",
    "  import jaxopt\n",
    "try:\n",
    "  import optax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install optax\n",
    "  import optax\n",
    "try:\n",
    "  import sklearn\n",
    "except ModuleNotFoundError:\n",
    "  %pip install sklearn\n",
    "  import sklearn\n",
    "\n",
    "try:\n",
    "  import torch\n",
    "except ModuleNotFoundError:\n",
    "  %pip install torch\n",
    "  import torch\n",
    "\n",
    "try:\n",
    "  import gpytorch\n",
    "except ModuleNotFoundError:\n",
    "  %pip install gpytorch\n",
    "  import gpytorch\n",
    "\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from tinygp import kernels, GaussianProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(theta,x,y):\n",
    "  kernel = jnp.exp(theta[\"log_varf\"])*kernels.ExpSquared(scale=jnp.exp(theta[\"log_scale\"]))\n",
    "  k = kernel(x,x) + ( jnp.exp(theta[\"log_vary\"]) * jnp.eye(len(x)))\n",
    "  mean_vec= jnp.zeros(y.shape[0])\n",
    "  dist = distrax.MultivariateNormalFullCovariance(mean_vec, k)\n",
    "  dist_logprob = dist.log_prob(y.reshape(-1,))\n",
    "  return -dist_logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIny GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gp(theta_, x):\n",
    "  kernel = jnp.exp(theta_[\"log_varf\"])*kernels.ExpSquared(scale=jnp.exp(theta_[\"log_scale\"]))\n",
    "  return GaussianProcess(kernel, x, diag = jnp.exp(theta_[\"log_vary\"]))\n",
    "\n",
    "def Nll(theta_, x, y):\n",
    "  gp = build_gp(theta_, x)\n",
    "  return -gp.log_probability(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x, y, theta, batch_size, alpha, epochs):\n",
    "  nll_epoch = []\n",
    "  var_signal = []\n",
    "  var_noise = []\n",
    "  theta1 = theta\n",
    "  lr = alpha\n",
    "  nll_gradient = (jax.grad(Nll,  argnums = 0))\n",
    "  \n",
    "  ## determining the number of batches\n",
    "  if (len(x) % batch_size  == 0):\n",
    "    num_batches = int(len(x)/batch_size)\n",
    "  else:\n",
    "    num_batches = int((len(x)/batch_size)) + 1\n",
    "\n",
    "  \n",
    "  ## finiding NN indices\n",
    "  neigh = NearestNeighbors(n_neighbors=batch_size, algorithm='kd_tree')\n",
    "  neigh.fit(x)\n",
    "  _,neigh_idx = neigh.kneighbors(x, batch_size)\n",
    "  tx = optax.adam(lr)\n",
    "    \n",
    "  for i in range(epochs):\n",
    "    \n",
    "    ## use tfds for shuffling\n",
    "    batch_index = 0\n",
    "    # X_, Y_ = jax.random.shuffle(jax.random.PRNGKey(2), x), jax.random.shuffle(jax.random.PRNGKey(2), y)\n",
    "\n",
    "  \n",
    "    for k in range(num_batches):\n",
    "      \n",
    "    \n",
    "      opt_state2 = tx.init((theta1[\"log_varf\"]))\n",
    "      opt_state3 = tx.init((theta1[\"log_vary\"]))\n",
    "\n",
    "      ## Random batches\n",
    "      # if batch_index+batch_size > len(X_):\n",
    "      #    X_batch, Y_batch = X_[batch_index:,:], Y_[batch_index:,:]\n",
    "      # else:\n",
    "      #   X_batch, Y_batch = X_[batch_index:batch_index+batch_size,:], Y_[batch_index:batch_index+batch_size,:]\n",
    "\n",
    "      # grads = nll_gradient(theta1, X_batch, Y_batch)\n",
    "      \n",
    "      ## NN batches\n",
    "      # center_idx  = jax.random.randint(jax.random.PRNGKey(0),(1,), 1, len(y))\n",
    "      center_idx = torch.tensor((k - 1)%len(y))\n",
    "      nn_batch_indices =  neigh_idx[center_idx, ]\n",
    "      nn_batch_X  = x[nn_batch_indices, ].reshape(-1,1)\n",
    "      nn_batch_y  = y[nn_batch_indices,].reshape(-1,1)\n",
    "      \n",
    "      grads = nll_gradient(theta1,  nn_batch_X,  nn_batch_y)\n",
    "\n",
    "\n",
    "      ## updating params\n",
    "      updates2,opt_state2 = tx.update(((batch_size*grads[\"log_varf\"])/(3*jnp.log(batch_size))), opt_state2)\n",
    "      theta1[\"log_varf\"] = optax.apply_updates((theta1[\"log_varf\"]), updates2)\n",
    "      updates3,opt_state3 = tx.update(grads[\"log_vary\"], opt_state3)\n",
    "      theta1[\"log_vary\"] = optax.apply_updates((theta1[\"log_vary\"]), updates3)\n",
    "      nll_epoch.append(Nll(theta1,x.reshape(-1,), y.reshape(-1,)))\n",
    "      var_signal.append(jnp.exp(theta1[\"log_varf\"]))\n",
    "      var_noise.append(jnp.exp(theta1[\"log_vary\"]))\n",
    "\n",
    "      # print(jnp.exp(theta1[\"log_varf\"]), jnp.exp(theta1[\"log_vary\"]))\n",
    "      batch_index += batch_size\n",
    "      # lr = lr/(k+1)\n",
    "\n",
    "  print(Nll(theta1,x.reshape(-1,), y.reshape(-1,)))\n",
    "  print(jnp.exp(theta1[\"log_scale\"]), jnp.exp(theta1[\"log_varf\"]), jnp.exp(theta1[\"log_vary\"]))\n",
    "\n",
    "  return nll_epoch, var_signal, var_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "N = 1024\n",
    "\n",
    "theta_init1 = {\"log_varf\": jnp.log(5.),\"log_vary\": jnp.log(3.),\"log_scale\": jnp.log(0.01)} \n",
    "theta_init2 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(3.5),\"log_scale\": jnp.log(0.01)}\n",
    "theta_init3 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(0.7),\"log_scale\": jnp.log(0.01)}\n",
    "\n",
    "theta_init = [theta_init1, theta_init2, theta_init3]\n",
    "alpha_arr = [0.01, 0.01, 0.01]\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(60,12))\n",
    "\n",
    "seed = np.random.randint(50, size=10)\n",
    "\n",
    "for  j in range(3):\n",
    "  \n",
    "  for t in tqdm(range(10)):\n",
    "    theta_init1 = {\"log_varf\": jnp.log(5.),\"log_vary\": jnp.log(3.),\"log_scale\": jnp.log(0.01)} \n",
    "    theta_init2 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(3.5),\"log_scale\": jnp.log(0.01)}\n",
    "    theta_init3 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(0.7),\"log_scale\": jnp.log(0.01)}\n",
    "\n",
    "    theta_init = [theta_init1, theta_init2, theta_init3]\n",
    "\n",
    "    theta_ = theta_init[j]\n",
    "    # print(theta_)\n",
    "    alpha_ = alpha_arr[j] \n",
    "\n",
    "    # key = jax.random.PRNGKey(seed[t])\n",
    "\n",
    "    # X_dist = distrax.Normal(jnp.array(0.0),jnp.array(5.0))\n",
    "    # X = X_dist.sample(seed=key, sample_shape = (1024,)).reshape(-1,1)\n",
    "    # key_ = jax.random.split(key, num=3)\n",
    "    # # print(X.shape)\n",
    "\n",
    "    # varf = jnp.array(4.0)\n",
    "    # len_scale = jnp.array(0.01)\n",
    "    # vary = jnp.array(0.01)\n",
    "\n",
    "    # kernel = varf*kernels.ExpSquared(scale=len_scale)\n",
    "    # cov = kernel(X, X) + vary * jnp.eye(len(X))\n",
    "    # # cov = varf*kernels.ExpSquared(scale=len_scale) \n",
    "    # mean_vec = jnp.zeros(N,)\n",
    "    # Y_dist = distrax.MultivariateNormalFullCovariance(mean_vec, cov)\n",
    "    # Y = Y_dist.sample(seed=key_[0]).reshape(-1,1)\n",
    "    # print(Y.shape)\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    x_dist = Normal(torch.tensor([0.0]), torch.tensor([5.0]))\n",
    "    X = x_dist.sample((N,))\n",
    "    K = ScaleKernel(RBFKernel())\n",
    "    K.base_kernel.lengthscale = 0.01\n",
    "    K.outputscale = 1.0\n",
    "    cov = K(X,X) + (0.1)*(torch.eye(len(X)))\n",
    "    dist = MultivariateNormal(torch.zeros((1024)),cov.evaluate())\n",
    "    torch.manual_seed(t+1)\n",
    "    Y = dist.sample()\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    X = jnp.array(X)\n",
    "\n",
    "    Y = np.asarray(Y)\n",
    "    Y = jnp.array(Y)\n",
    "\n",
    "\n",
    "    loss, param1, param2 = SGD(X, Y, theta_, batch_size, alpha_, 50)\n",
    "\n",
    "    # ax[].plot(loss, label='loss')\n",
    "    ax[j].plot(param1, linestyle='dashed', linewidth=1, markersize=8)\n",
    "    ax[j].plot(param2, linestyle='dotted', linewidth=1, markersize=6)\n",
    "\n",
    "  ax[j].axhline(y = 1, color = 'black', linestyle = 'dashed')\n",
    "  ax[j].axhline(y = 0.1, color = 'black', linestyle = ':')\n",
    "  ax[j].set_xlabel('Iteration (k)')\n",
    "  ax[j].set_ylabel('O(k)')\n",
    "  \n",
    "  theta_init1 = {\"log_varf\": jnp.log(5.),\"log_vary\": jnp.log(3.),\"log_scale\": jnp.log(0.01)} \n",
    "  theta_init2 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(3.5),\"log_scale\": jnp.log(0.01)}\n",
    "  theta_init3 = {\"log_varf\": jnp.log(2.5),\"log_vary\": jnp.log(2.7),\"log_scale\": jnp.log(0.01)}\n",
    "\n",
    "  theta_init = [theta_init1, theta_init2, theta_init3]\n",
    "\n",
    "  ax[j].set_title(f'({round(jnp.exp(theta_init[j][\"log_varf\"]).item(),2)},{round(jnp.exp(theta_init[j][\"log_vary\"]).item(),2)})')\n",
    "  ax[j].set_xticks([0,50,100,150,200])\n",
    "  ax[j].set_yticks([0,1,2])\n",
    "  ax[j].grid()\n",
    " \n",
    "  \n",
    "plt.savefig('figure1_adam0.01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SGDGP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ac03bc53b81f274766a4c3d65a8bb144021040a3f00168f019aaf215aad8e7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
