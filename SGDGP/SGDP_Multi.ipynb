{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import distrax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install distrax\n",
    "  import distrax\n",
    "try:\n",
    "  import jax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install jax \n",
    "  import jax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "  %pip install matplotlib \n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "try:\n",
    "  import GPy\n",
    "except ModuleNotFoundError:\n",
    "  %pip install GPy\n",
    "  import GPy\n",
    "\n",
    "try:\n",
    "  from tqdm import tqdm\n",
    "except ModuleNotFoundError:\n",
    "  %pip install tqdm\n",
    "  from tqdm import tqdm\n",
    "\n",
    "\n",
    "try: \n",
    "  import jaxopt\n",
    "except ModuleNotFoundError:\n",
    "  %pip install jaxopt\n",
    "  import jaxopt\n",
    "\n",
    "import optax\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "try:\n",
    "  from pyDOE import *\n",
    "except ModuleNotFoundError:\n",
    "  ! pip install pyDOE\n",
    "  from pyDOE import *\n",
    "\n",
    "try:\n",
    "  from smt.sampling_methods import LHS\n",
    "except ModuleNotFoundError:\n",
    "   ! pip install smt\n",
    "   from smt.sampling_methods import LHS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimits = np.array([[-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0], [-10.0, 10.0]])\n",
    "random_state = np.random.seed(0)\n",
    "sampling = LHS(xlimits=xlimits, random_state=0 )\n",
    "num = 10000\n",
    "# print(num)\n",
    "\n",
    "x = sampling(num)\n",
    "X = pd.DataFrame(x)\n",
    "# print(0)\n",
    "w = []\n",
    "for ii in X.columns:\n",
    "    w.append(1 + (X[ii] - 1) / 4)\n",
    "\n",
    "term1 = (np.sin(np.pi * w[0])) ** 2\n",
    "term3 = (w[-1] - 1) ** 2 * (1 + (np.sin(2 * np.pi * w[-1])) ** 2)\n",
    "\n",
    "sum = 0\n",
    "for ii in range(len(X.columns) - 1):\n",
    "    wi = w[ii]\n",
    "    new = (wi - 1) ** 2 * (1 + 10 * (np.sin(np.pi * wi + 1)) ** 2)\n",
    "    sum = sum + new\n",
    "y = term1 + sum + term3\n",
    "  # return X, y\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0\n",
    ")\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "y_train = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test = scaler.transform(np.array(y_test).reshape(-1, 1))\n",
    "X_train.to_csv(\"Datasets/Levy/X_train.csv.gz\")\n",
    "X_test.to_csv(\"Datasets/Levy/X_test.csv.gz\")\n",
    "pd.DataFrame(y_train).to_csv(\"Datasets/Levy/y_train.csv.gz\")\n",
    "pd.DataFrame(y_test).to_csv(\"Datasets/Levy/y_test.csv.gz\")\n",
    "# X_train, X_test, y_train, y_test = torch.tensor(np.array([X_train])).squeeze(),torch.tensor(np.array([X_test])).squeeze(),torch.tensor(np.array([y_train])).squeeze(),torch.tensor(np.array([y_test])).squeeze()\n",
    "# return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"Datasets/Levy\"\n",
    "X_train = pd.read_csv(dirname+\"/X_train.csv.gz\").iloc[:,1:]\n",
    "X_test = pd.read_csv(dirname+\"/X_test.csv.gz\").iloc[:,1:]\n",
    "y_train = pd.read_csv(dirname+\"/y_train.csv.gz\").iloc[:,1:]\n",
    "y_test = pd.read_csv(dirname+\"/y_test.csv.gz\").iloc[:,1:]\n",
    "\n",
    "X_train, X_test = jnp.array(jnp.array([X_train])).squeeze(),jnp.array([X_test]).squeeze()\n",
    "y_train, y_test = jnp.array([y_train]).squeeze(),jnp.array([y_test]).squeeze() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rbf(self, x, x_star, len_scale, varf):\n",
    "#   sqdist = jnp.sum(x**2,1) + jnp.sum(x_star**2,1) - 2*jnp.dot(x, x_star.T)\n",
    "#   return varf*jnp.exp(-.5 * (1/len_scale) * sqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_samples(x, key):\n",
    "\n",
    "  # def rbf(x, x_star, len_scale, sigmaf):\n",
    "  #   return (sigmaf**2)*(jnp.exp(-(x-x_star)**2/(2*(len_scale**2))))  \n",
    "\n",
    "  def rbf(x, x_star, len_scale, varf):\n",
    "    sqdist = jnp.sum(x**2,1).reshape(-1,1) + jnp.sum(x_star**2,1) - 2*jnp.dot(x, x_star.T)\n",
    "    return varf*jnp.exp(-.5 * (1/len_scale) * sqdist)\n",
    "    \n",
    "  cov = rbf(x, x, 0.1, 1)\n",
    "  mean_vec = jnp.zeros(x.shape[0])\n",
    "  prior = distrax.MultivariateNormalFullCovariance(mean_vec,cov)\n",
    "  f_prior = prior.sample(seed=key, sample_shape = (10,))\n",
    "  f_prior = f_prior.T\n",
    "  return f_prior\n",
    "\n",
    "f_prior = prior_samples(X_train, key)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(X_train[:,0], f_prior)\n",
    "plt.title(\"10 prior samples \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP_Regression:\n",
    "\n",
    "  # def rbf(self, x, x_star, len_scale,varf):\n",
    "  #   return (varf)*(jnp.exp(-(x-x_star)**2/(2*(len_scale**2))))  \n",
    "\n",
    "  def rbf(self, x, x_star, len_scale, varf):\n",
    "    sqdist = jnp.sum(x**2,1).reshape(-1,1) + jnp.sum(x_star**2,1) - 2*jnp.dot(x, x_star.T)\n",
    "    return varf*jnp.exp(-.5 * (1/len_scale) * sqdist)\n",
    "    \n",
    "  def periodic_kernel(self, x, xstar, len_scale, varf, period):\n",
    "    return (varf)*(jnp.exp((-1/2*(len_scale**2)) * jnp.square(jnp.sin(jnp.pi * (x-xstar)/period))))  \n",
    "\n",
    "\n",
    "  def train(self,theta,data):\n",
    "    X, y = data\n",
    "    self.len_scale = jnp.exp(theta[\"log_scale\"])\n",
    "    self.varf = jnp.exp(theta[\"log_varf\"])\n",
    "    k = self.rbf(X,X,self.len_scale, self.varf)\n",
    "    self.k = k + ( jnp.exp(theta[\"log_vary\"]) * jnp.eye(len(X)))\n",
    "    mean_vec= jnp.zeros(y.shape[0])\n",
    "    dist = distrax.MultivariateNormalFullCovariance(mean_vec, self.k)\n",
    "    dist_logprob = dist.log_prob(y.reshape(-1,))\n",
    "    return -dist_logprob\n",
    "  \n",
    "\n",
    "  def posterior(self, X, y, X_test, mu_prior):\n",
    "    L = jnp.linalg.cholesky(self.k)\n",
    "    kstar = self.rbf(X, X_test, self.len_scale, self.varf)\n",
    "    kstar_star = self.rbf(X_test, X_test, self.len_scale, self.varf)\n",
    "    # posterior mean\n",
    "    alpha = jnp.linalg.solve(L.T, (jnp.linalg.solve(L, (y - mu_prior))))\n",
    "    self.mu_posterior =  mu_prior + jnp.dot(kstar.T, alpha)\n",
    "    # posterior covarinace\n",
    "    v = jnp.linalg.solve(L, kstar)\n",
    "    self.cov_posterior = kstar_star - jnp.dot(v.T, v)\n",
    "    return self.mu_posterior, self.cov_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(X, m, C, training_points=None):\n",
    "    \"\"\" Plotting utility to plot a GP fit with 95% confidence interval \"\"\"\n",
    "    # Plot 95% confidence interval \n",
    "    plt.fill_between(X[:,0],\n",
    "                     m[:,0] - 1.96*jnp.sqrt(jnp.diag(C)),\n",
    "                     m[:,0] + 1.96*jnp.sqrt(jnp.diag(C)),\n",
    "                     alpha=0.5)\n",
    "    # Plot GP mean and initial training points\n",
    "    plt.plot(X, m, \"-\")\n",
    "    plt.legend(labels=[\"GP fit\"])\n",
    "    \n",
    "    plt.xlabel(\"x\"), plt.ylabel(\"f\")\n",
    "    \n",
    "    # Plot training points if included\n",
    "    if training_points is not None:\n",
    "        X_, Y_ = training_points\n",
    "        plt.plot(X_, Y_, \"kx\", mew=2)\n",
    "        plt.legend(labels=[\"GP fit\", \"sample points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GP_Regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m GPR \u001b[39m=\u001b[39m GP_Regression()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m theta_init \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlog_varf\u001b[39m\u001b[39m\"\u001b[39m: jnp\u001b[39m.\u001b[39mlog(\u001b[39m1.\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlog_vary\u001b[39m\u001b[39m\"\u001b[39m: jnp\u001b[39m.\u001b[39mlog(\u001b[39m1.\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlog_scale\u001b[39m\u001b[39m\"\u001b[39m: jnp\u001b[39m.\u001b[39mlog(\u001b[39m1.\u001b[39m)}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/SGD_Gaussian_Process/SGDGP/SGDP_Multi.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m \u001b[39m#101.502873968226,121454.38823926545, 4.5781156963414125\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GP_Regression' is not defined"
     ]
    }
   ],
   "source": [
    "GPR = GP_Regression()\n",
    "\n",
    "theta_init = {\n",
    "  \"log_varf\": jnp.log(1.),\n",
    "  \"log_vary\": jnp.log(1.),\n",
    "  \"log_scale\": jnp.log(1.)}\n",
    "\n",
    "#101.502873968226,121454.38823926545, 4.5781156963414125\n",
    "initial_nll = GPR.train(theta_init,(X_train, y_train))\n",
    "print(initial_nll)\n",
    "Xnew = jnp.vstack([X_train, X_test])\n",
    "print(Xnew.shape)\n",
    "mu_posterior, cov_posterior = GPR.posterior(X_train, y_train, Xnew, 0)\n",
    "print(mu_posterior.shape, cov_posterior.shape)\n",
    "\n",
    "noise_matrix =  jnp.exp(theta_init[\"log_vary\"])*jnp.eye(cov_posterior.shape[0],cov_posterior.shape[1])\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_gp(Xnew, mu_posterior.reshape(-1,1), cov_posterior+noise_matrix)\n",
    "plt.plot(X_train, y_train, \"b.\")\n",
    "plt.savefig(\"levy_post.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial paramters\n",
    "max_iters = int(1000)\n",
    "lr = 0.01\n",
    "nll_iters = []\n",
    "\n",
    "theta_init = {\n",
    "  \"log_varf\": jnp.log(1.),\n",
    "  \"log_vary\": jnp.log(1.),\n",
    "  \"log_scale\": jnp.log(1.)}\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "nll_gradient = jax.jit(jax.value_and_grad(GPR.train, argnums = 0))\n",
    "\n",
    "# Using adam optimizer\n",
    "\n",
    "## Nearest Neighbour calculation\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=32, algorithm='kd_tree')\n",
    "neigh.fit(X_train)\n",
    "_,neigh_idx = neigh.kneighbors(X_train, 32)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(14,6))\n",
    "\n",
    "for j in range(1):\n",
    " \n",
    "  if (len(X_train)%batch_size  == 0):\n",
    "    num_batches = int(len(X_train)/batch_size)\n",
    "  else:\n",
    "    num_batches = int((len(X_train)/batch_size)) + 1\n",
    "\n",
    "\n",
    "  for i in tqdm(range(max_iters)):\n",
    "    \n",
    "    ## use tfds for shuffling\n",
    "    X_, Y_ = jax.random.shuffle(key, X_train), jax.random.shuffle(key, y_train)\n",
    "    # print(X_.shape,Y_.shape)\n",
    "    Y_ =Y_.reshape(-1,1)\n",
    "    batch_index = 0\n",
    "  \n",
    "    for k in range(num_batches):\n",
    "\n",
    "      tx1 = optax.adam(lr)\n",
    "      opt_state1 = tx1.init((theta_init[\"log_scale\"]))\n",
    "      # tx2 = optax.adam(lr)\n",
    "      opt_state2 = tx1.init((theta_init[\"log_varf\"]))\n",
    "      opt_state3 = tx1.init((theta_init[\"log_vary\"]))\n",
    "\n",
    "      ## Random batches\n",
    "      if batch_index+batch_size > len(X_):\n",
    "         X_batch, Y_batch = X_[batch_index:,:], Y_[batch_index:,:]\n",
    "      else:\n",
    "        X_batch, Y_batch = X_[batch_index:batch_index+batch_size,:], Y_[batch_index:batch_index+batch_size,:]\n",
    "      ## NN batches\n",
    "      center_idx  = jax.random.randint(key,(1,), 1, len(y_train))\n",
    "      nn_batch_indices =  neigh_idx[center_idx,]\n",
    "      nn_batch_X  = X_train[nn_batch_indices,].reshape(-1,1)\n",
    "      nn_batch_y  = y_train[nn_batch_indices,].reshape(-1,1)\n",
    "      batch_index += batch_size\n",
    "      nn_batch_X = nn_batch_X.reshape(-1,4)\n",
    "      loss_val,grads_rs = nll_gradient(theta_init,(X_batch, Y_batch))\n",
    "      loss_val,grads_nn = nll_gradient(theta_init,(nn_batch_X, nn_batch_y))\n",
    "      nll_iters.append(GPR.train(theta_init,(X_train, y_train)))\n",
    "      updates1,opt_state1 = tx1.update(grads_rs[\"log_scale\"], opt_state1)\n",
    "      theta_init[\"log_scale\"] = optax.apply_updates((theta_init[\"log_scale\"]), updates1)\n",
    "      updates2,opt_state2 = tx1.update(((batch_size*grads_rs[\"log_varf\"])/(3*jnp.log(batch_size))), opt_state2)\n",
    "      theta_init[\"log_varf\"] = optax.apply_updates((theta_init[\"log_varf\"]), updates2)\n",
    "      updates3,opt_state3 = tx1.update(grads_nn[\"log_vary\"], opt_state3)\n",
    "      theta_init[\"log_vary\"] = optax.apply_updates((theta_init[\"log_vary\"]), updates3)\n",
    "\n",
    "\n",
    "  print(GPR.train(theta_init,(X_train, y_train)))\n",
    "  plt.plot(nll_iters)\n",
    "  plt.savefig(\"loss.png\")\n",
    "  \n",
    "print(jnp.exp(theta_init[\"log_scale\"]), jnp.exp(theta_init[\"log_varf\"]), jnp.exp(theta_init[\"log_vary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SGDGP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ac03bc53b81f274766a4c3d65a8bb144021040a3f00168f019aaf215aad8e7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
